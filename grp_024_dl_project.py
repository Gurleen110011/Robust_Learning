# -*- coding: utf-8 -*-
"""grp_024_DL_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ttoxfx0wafdvgWe84ItgZzE1WVr9FP4M

#Case1 Figure(2, 3a, 6a)
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import math
import numpy as np
import matplotlib.pyplot as plt
from math import log, e
import torch.optim as optim
import pickle
import sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

torch.manual_seed(22)
np.random.seed(22)

DIR_NAME = 'inner_prod/'
SAVE=True
GROUP_SIZE=0.98
NOISE=1/8

class ConvNet(nn.Module):
    def __init__(self, input_dim, out_channel, patch_num, small=True):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(1, out_channel*2, int(input_dim/patch_num), int(input_dim/patch_num))
        if small:
            self.conv1.weight = torch.nn.Parameter(self.conv1.weight*0.1)
            self.conv1.bias = torch.nn.Parameter(self.conv1.bias*0.1)
        self.out_channel = out_channel

    def forward(self, x):
        x = self.conv1(x)
        x = x**3
        x = torch.sum(x,2)
        output = torch.stack([torch.sum(x[:,:self.out_channel],1), torch.sum(x[:,self.out_channel:],1)]).transpose(1,0)
        return output

def train(model, criterion, data, labels, optimizers, epochs=100, plot=False, verbose=True):
    min_loss = 100
    core, spu, loss_list = [], [], []
    core_inner, spu_inner = test_inner(model)
    core.append(core_inner)
    spu.append(spu_inner)

    for epoch in range(epochs):
        for optimizer in optimizers:
            optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss_list.append(loss.detach())
        loss.backward()
        core_inner, spu_inner = test_inner(model)
        core.append(core_inner)
        spu.append(spu_inner)

        for optimizer in optimizers:
            optimizer.step()

        if epoch%100 == 0:
            if verbose:
                print('Epoch %d --- loss: %.3f' % (epoch + 1, loss.item()))

    core, spu, loss_list = torch.stack(core), torch.stack(spu), torch.stack(loss_list)
    print('Finished Training')
    return core, spu, loss_list


def test(model, criterion, data, labels):
    correct = 0

    with torch.no_grad():
        outputs = model(data)
        predicted = torch.max(outputs.data, 1).indices
        correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the %d test images: %.4f %%' % (data.shape[0],
        100 * correct / data.shape[0]))

    return 100 * correct / data.shape[0]


def test_inner(model):
    with torch.no_grad():
        core_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                      vc.float().unsqueeze(1).cuda())))
        spurious_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                          vs.float().unsqueeze(1).cuda())))

    return core_inner, spurious_inner

DATA_NUM = 10000
PATCH_NUM = 3
PATCH_LEN = 50

def div_norm(x):
    x /= np.linalg.norm(x)
    return x

vc = np.random.randn(PATCH_LEN)
vc /= np.linalg.norm(vc)

vs = np.random.randn(PATCH_LEN)
vs /= np.linalg.norm(vs)
vs -= vs.dot(vc) * vc
vs /= np.linalg.norm(vs)

vc, vs = torch.tensor(vc), torch.tensor(vs)

data = []
data_visual = []
labels = []
spurious_labels = []

xi_large, xi_small = torch.zeros(PATCH_LEN), torch.zeros(PATCH_LEN)

for i in range(DATA_NUM*2):
    y = np.random.choice([-1,1], 1)[0]
    alpha = np.random.uniform(0,1)

    # Feature noise patch
    a = (alpha<GROUP_SIZE)*y - (alpha>=GROUP_SIZE)*y
    # Noise patch
    xi = torch.tensor(np.random.normal(0, NOISE, size=(PATCH_LEN)))

    if a==y:
        xi_large+=xi
    else:
        xi_small+=xi

    core = vc*y/5
    spurious = vs*a
    x = torch.stack([core, spurious, xi])

    x_visual = torch.stack([core*1.2, spurious, xi])

    x = x.flatten()
    x_visual = x_visual.flatten()

    data.append(x)
    data_visual.append(x_visual)
    labels.append(y)
    spurious_labels.append(a)

data = torch.stack(data)
data_visual = torch.stack(data_visual)
print(data.shape)

labels = torch.tensor(labels)
spurious_labels = torch.tensor(spurious_labels)
labels[labels==-1] = 0
spurious_labels[spurious_labels==-1]=0
print(labels.shape)

training_data = data[:DATA_NUM,:].unsqueeze(1).float().cuda()
test_data = data[DATA_NUM:,:].unsqueeze(1).float().cuda()

print(training_data.shape, test_data.shape)

training_labels = labels[:DATA_NUM].cuda()
test_labels = labels[DATA_NUM:].cuda()
spurious_training_labels = spurious_labels[:DATA_NUM].cuda()
spurious_test_labels = spurious_labels[DATA_NUM:].cuda()
print(training_labels.shape, test_labels.shape)

training_labels = training_labels.long()
test_labels = test_labels.long()
spurious_training_labels = spurious_training_labels.long()
spurious_test_labels = spurious_test_labels.long()

pca = TSNE(n_components=2)

data_visual= pca.fit_transform(data_visual.float().cpu())
data_visual = data_visual/np.max(np.abs(data_visual))

visual_data_train = data_visual[:DATA_NUM,:]
visual_data_test = data_visual[DATA_NUM:,:]

plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.legend(["y=-1","y=+1"])

#if SAVE:
    #plt.savefig(DIR_NAME+"synthetic.pdf")

plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

plt.scatter(visual_data_test[spurious_test_labels.cpu()==0][:,0],visual_data_test[spurious_test_labels.cpu()==0][:,1], s=20, marker='o',color='#fdae61')
plt.scatter(visual_data_test[spurious_test_labels.cpu()==1][:,0],visual_data_test[spurious_test_labels.cpu()==1][:,1], s=20, marker='o',color='#abdda4')
plt.legend(["a=-1","a=+1"])

#if SAVE:
    #plt.savefig(DIR_NAME+"synthetic2.pdf")

for i in range(DATA_NUM):
    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([training_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    training_data[i,0,:] = stack_data

    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([test_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    test_data[i,0,:] = stack_data

small_test_data = torch.concat([test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])
small_test_label = torch.concat([test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])

num_epochs = 401

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda()
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.1)

core, spu, loss_list = train(CNN, criterion, training_data, training_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

plt.plot(spu.cpu(),color="tab:blue",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_s\rangle$')
plt.plot(core.cpu(),color="tab:orange",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_c\rangle$')
plt.plot(loss_list.cpu(),color="tab:gray",linewidth=4,linestyle='--',label='training loss')

plt.legend(prop={'size': 20},loc='center left')

#if SAVE:
    #plt.savefig(DIR_NAME+"synthetic_GD.pdf", bbox_inches='tight')

pred = CNN(test_data)

resolution = 100 # 100x100 background pixels
X2d_xmin, X2d_xmax = np.min(visual_data_test[:,0]), np.max(visual_data_test[:,0])
X2d_ymin, X2d_ymax = np.min(visual_data_test[:,1]), np.max(visual_data_test[:,1])
xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))

# approximate Voronoi tesselation on resolution x resolution grid using 1-NN
background_model = KNeighborsClassifier(n_neighbors=1).fit(visual_data_test, torch.max(pred.data, 1).indices.cpu().numpy())
voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])
voronoiBackground = voronoiBackground.reshape((resolution, resolution))

#plot
plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.contour(xx, yy, voronoiBackground)
plt.legend(["y=-1","y=+1"])

#if SAVE:
    ##plt.savefig(DIR_NAME+"synthetic_dbtest_gd.pdf")

pred = CNN(test_data)

resolution = 100 # 100x100 background pixels
X2d_xmin, X2d_xmax = np.min(visual_data_test[:,0]), np.max(visual_data_test[:,0])
X2d_ymin, X2d_ymax = np.min(visual_data_test[:,1]), np.max(visual_data_test[:,1])
xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))

# approximate Voronoi tesselation on resolution x resolution grid using 1-NN
background_model = KNeighborsClassifier(n_neighbors=1).fit(visual_data_test, torch.max(pred.data, 1).indices.cpu().numpy())
voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])
voronoiBackground = voronoiBackground.reshape((resolution, resolution))

#plot
plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.contour(xx, yy, voronoiBackground)
plt.legend(["y=-1","y=+1"],prop={'size': 20})

#if SAVE:
#    #plt.savefig(DIR_NAME+"synthetic_dbtest_gdm.pdf")

num_epochs = 401

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda()
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.03, momentum=0.8)

core, spu, loss_list = train(CNN, criterion, training_data, training_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

plt.plot(spu.cpu(),color="tab:blue",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_s\rangle$')
plt.plot(core.cpu(),color="tab:orange",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_c\rangle$')
plt.plot(loss_list.cpu(),color="tab:gray",linewidth=4,linestyle='--',label='training loss')
plt.legend(prop={'size': 20})

''' '''#if SAVE:
#plt.savefig(DIR_NAME+"synthetic_GDM.pdf", bbox_inches='tight')''''''

"""### Case 2 (Figure 3b, 6b)"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import math
import numpy as np
import matplotlib.pyplot as plt
from math import log, e
import torch.optim as optim
import pickle
import sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

torch.manual_seed(22)
np.random.seed(22)

DIR_NAME = 'inner_prod/'
SAVE=True
GROUP_SIZE=0.98
NOISE=1/8

class ConvNet(nn.Module):
    def __init__(self, input_dim, out_channel, patch_num, small=True):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(1, out_channel*2, int(input_dim/patch_num), int(input_dim/patch_num))
        if small:
            self.conv1.weight = torch.nn.Parameter(self.conv1.weight*0.1)
            self.conv1.bias = torch.nn.Parameter(self.conv1.bias*0.1)
        self.out_channel = out_channel

    def forward(self, x):
        x = self.conv1(x)
        x = x**3
        x = torch.sum(x,2)
        output = torch.stack([torch.sum(x[:,:self.out_channel],1), torch.sum(x[:,self.out_channel:],1)]).transpose(1,0)
        return output

def train(model, criterion, data, labels, optimizers, epochs=100, plot=False, verbose=True):
    min_loss = 100
    core, spu, loss_list = [], [], []
    core_inner, spu_inner = test_inner(model)
    core.append(core_inner)
    spu.append(spu_inner)


    for epoch in range(epochs):

        for optimizer in optimizers:
            optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss_list.append(loss.detach())
        loss.backward()
        core_inner, spu_inner = test_inner(model)
        core.append(core_inner)
        spu.append(spu_inner)


        for optimizer in optimizers:
            optimizer.step()

        if epoch%100 == 0:
            if verbose:
                print('Epoch %d --- loss: %.3f' % (epoch + 1, loss.item()))

    core, spu, loss_list = torch.stack(core), torch.stack(spu), torch.stack(loss_list)
    print('Finished Training')
    return core, spu, loss_list


def test(model, criterion, data, labels):
    correct = 0

    with torch.no_grad():
        outputs = model(data)
        predicted = torch.max(outputs.data, 1).indices
        correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the %d test images: %.4f %%' % (data.shape[0],
        100 * correct / data.shape[0]))

    return 100 * correct / data.shape[0]


def test_inner(model):
    with torch.no_grad():
        core_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                      vc.float().unsqueeze(1).cuda())))
        spurious_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                          vs.float().unsqueeze(1).cuda())))
    return core_inner, spurious_inner

DATA_NUM = 10000
PATCH_NUM = 3
PATCH_LEN = 50

def div_norm(x):
    x /= np.linalg.norm(x)
    return x

vc = np.random.randn(PATCH_LEN)
vc /= np.linalg.norm(vc)

vs = np.random.randn(PATCH_LEN)
vs /= np.linalg.norm(vs)
vs -= vs.dot(vc) * vc
vs /= np.linalg.norm(vs)

vc, vs = torch.tensor(vc), torch.tensor(vs)

data = []
data_visual = []
labels = []
spurious_labels = []

xi_large, xi_small = torch.zeros(PATCH_LEN), torch.zeros(PATCH_LEN)

for i in range(DATA_NUM*2):
    y = np.random.choice([-1,1], 1)[0]
    alpha = np.random.uniform(0,1)
    # Feature noise patch
    a = (alpha<GROUP_SIZE)*y - (alpha>=GROUP_SIZE)*y
    # Noise patch
    xi = torch.tensor(np.random.normal(0, NOISE, size=(PATCH_LEN)))

    if a==y:
        xi_large+=xi
    else:
        xi_small+=xi

    core = vc*y
    spurious = vs*a/5
    x = torch.stack([core, spurious, xi])

    x_visual = torch.stack([core*1.2, spurious, xi])

    x = x.flatten()
    x_visual = x_visual.flatten()

    data.append(x)
    data_visual.append(x_visual)
    labels.append(y)
    spurious_labels.append(a)

data = torch.stack(data)
data_visual = torch.stack(data_visual)
print(data.shape)

labels = torch.tensor(labels)
spurious_labels = torch.tensor(spurious_labels)
labels[labels==-1] = 0
spurious_labels[spurious_labels==-1]=0
print(labels.shape)

training_data = data[:DATA_NUM,:].unsqueeze(1).float().cuda()
test_data = data[DATA_NUM:,:].unsqueeze(1).float().cuda()

print(training_data.shape, test_data.shape)

training_labels = labels[:DATA_NUM].cuda()
test_labels = labels[DATA_NUM:].cuda()
spurious_training_labels = spurious_labels[:DATA_NUM].cuda()
spurious_test_labels = spurious_labels[DATA_NUM:].cuda()
print(training_labels.shape, test_labels.shape)

training_labels = training_labels.long()
test_labels = test_labels.long()
spurious_training_labels = spurious_training_labels.long()
spurious_test_labels = spurious_test_labels.long()

for i in range(DATA_NUM):
    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([training_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    training_data[i,0,:] = stack_data

    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([test_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    test_data[i,0,:] = stack_data

small_test_data = torch.concat([test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])
small_test_label = torch.concat([test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])

num_epochs = 401

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda()
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.1)

core, spu, loss_list = train(CNN, criterion, training_data, training_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

plt.plot(spu.cpu(),color="tab:blue",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_s\rangle$')
plt.plot(core.cpu(),color="tab:orange",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_c\rangle$')
plt.plot(loss_list.cpu(),color="tab:gray",linewidth=4,linestyle='--',label='training loss')

##if SAVE:
    ##plt.savefig(DIR_NAME+"synthetic_GD_s2.pdf", bbox_inches='tight')

num_epochs = 401

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda()
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.03, momentum=0.8)

core, spu, loss_list = train(CNN, criterion, training_data, training_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

plt.plot(spu.cpu(),color="tab:blue",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_s\rangle$')
plt.plot(core.cpu(),color="tab:orange",linewidth=4,label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_c\rangle$')
plt.plot(loss_list.cpu(),color="tab:gray",linewidth=4,linestyle='--',label='training loss')

##if SAVE:
   # #plt.savefig(DIR_NAME+"synthetic_GDM_s2.pdf", bbox_inches='tight')

"""CASE1"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import math
import numpy as np
import matplotlib.pyplot as plt
from math import log, e
import torch.optim as optim
import pickle
import sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

torch.manual_seed(22)
np.random.seed(22)

DIR_NAME = 'PDE/'
SAVE=True
GROUP_SIZE=0.98
NOISE=1/8

class ConvNet(nn.Module):
    def __init__(self, input_dim, out_channel, patch_num, small=True):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(1, out_channel*2, int(input_dim/patch_num), int(input_dim/patch_num))
        if small:
            self.conv1.weight = torch.nn.Parameter(self.conv1.weight*0.1)
            self.conv1.bias = torch.nn.Parameter(self.conv1.bias*0.1)
        self.out_channel = out_channel

    def forward(self, x):
        x = self.conv1(x)
        x = x**3
        x = torch.sum(x,2)
        output = torch.stack([torch.sum(x[:,:self.out_channel],1), torch.sum(x[:,self.out_channel:],1)]).transpose(1,0)
        return output

def train(model, criterion, data, labels, optimizers, epochs=100, plot=False, verbose=True):
    min_loss = 100
    core, spu, loss_list = [], [], []
    core_inner, spu_inner = test_inner(model)
    core.append(core_inner)
    spu.append(spu_inner)

    for epoch in range(epochs):

        for optimizer in optimizers:
            optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss_list.append(loss.detach())
        loss.backward()
        core_inner, spu_inner = test_inner(model)
        core.append(core_inner)
        spu.append(spu_inner)

        for optimizer in optimizers:
            optimizer.step()

        if epoch%100 == 0:
            if verbose:
                print('Epoch %d --- loss: %.3f' % (epoch + 1, loss.item()))

    core, spu, loss_list = torch.stack(core), torch.stack(spu), torch.stack(loss_list)
    print('Finished Training')
    return core, spu, loss_list


def test(model, criterion, data, labels):
    correct = 0

    with torch.no_grad():
        outputs = model(data)
        predicted = torch.max(outputs.data, 1).indices
        correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the %d test images: %.4f %%' % (data.shape[0],
        100 * correct / data.shape[0]))

    return 100 * correct / data.shape[0]


def test_inner(model):
    with torch.no_grad():
        core_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                      vc.float().unsqueeze(1).cuda())))
        spurious_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                          vs.float().unsqueeze(1).cuda())))

    return core_inner, spurious_inner

DATA_NUM = 10000
PATCH_NUM = 3
PATCH_LEN = 50

def div_norm(x):
    x /= np.linalg.norm(x)
    return x

vc = np.random.randn(PATCH_LEN)
vc /= np.linalg.norm(vc)

vs = np.random.randn(PATCH_LEN)
vs /= np.linalg.norm(vs)
vs -= vs.dot(vc) * vc
vs /= np.linalg.norm(vs)

vc, vs = torch.tensor(vc), torch.tensor(vs)

data = []
data_visual = []
labels = []
spurious_labels = []

xi_large, xi_small = torch.zeros(PATCH_LEN), torch.zeros(PATCH_LEN)

for i in range(DATA_NUM*2):
    y = np.random.choice([-1,1], 1)[0]
    alpha = np.random.uniform(0,1)

    # Feature noise patch
    a = (alpha<GROUP_SIZE)*y - (alpha>=GROUP_SIZE)*y
    # Noise patch
    xi = torch.tensor(np.random.normal(0, NOISE, size=(PATCH_LEN)))

    if a==y:
        xi_large+=xi
    else:
        xi_small+=xi

    core = vc*y/5
    spurious = vs*a
    x = torch.stack([core, spurious, xi])

    x_visual = torch.stack([core*1.2, spurious, xi])

    x = x.flatten()
    x_visual = x_visual.flatten()

    data.append(x)
    data_visual.append(x_visual)
    labels.append(y)
    spurious_labels.append(a)

data = torch.stack(data)
data_visual = torch.stack(data_visual)
print(data.shape)

labels = torch.tensor(labels)
spurious_labels = torch.tensor(spurious_labels)
labels[labels==-1] = 0
spurious_labels[spurious_labels==-1]=0
print(labels.shape)

training_data = data[:DATA_NUM,:].unsqueeze(1).float().cuda()
test_data = data[DATA_NUM:,:].unsqueeze(1).float().cuda()

print(training_data.shape, test_data.shape)

training_labels = labels[:DATA_NUM].cuda()
test_labels = labels[DATA_NUM:].cuda()
spurious_training_labels = spurious_labels[:DATA_NUM].cuda()
spurious_test_labels = spurious_labels[DATA_NUM:].cuda()
print(training_labels.shape, test_labels.shape)

training_labels = training_labels.long()
test_labels = test_labels.long()
spurious_training_labels = spurious_training_labels.long()
spurious_test_labels = spurious_test_labels.long()

pca = TSNE(n_components=2)

data_visual= pca.fit_transform(data_visual.float().cpu())
data_visual = data_visual/np.max(np.abs(data_visual))

visual_data_train = data_visual[:DATA_NUM,:]
visual_data_test = data_visual[DATA_NUM:,:]

for i in range(DATA_NUM):
    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([training_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    training_data[i,0,:] = stack_data

    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([test_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    test_data[i,0,:] = stack_data

small_test_data = torch.concat([test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])
small_test_label = torch.concat([test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])
smallest_group_size = min(len(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]]),len(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]]))

warmup_data = []
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][:smallest_group_size])
warmup_data = torch.concat(warmup_data)

warmup_data.shape

gradual_data = []
gradual_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][smallest_group_size:])
gradual_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][smallest_group_size:])
gradual_data = torch.concat(gradual_data)

gradual_data.shape

warmup_labels = []
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][:smallest_group_size])
warmup_labels = torch.concat(warmup_labels)

warmup_labels.shape

gradual_labels = []
gradual_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][smallest_group_size:])
gradual_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][smallest_group_size:])
gradual_labels = torch.concat(gradual_labels)

gradual_labels.shape

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda()
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)

num_epochs = 801

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.03, momentum=0.9)

core, spu, loss_list = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

pred = CNN(test_data)

resolution = 100
X2d_xmin, X2d_xmax = np.min(visual_data_test[:,0]), np.max(visual_data_test[:,0])
X2d_ymin, X2d_ymax = np.min(visual_data_test[:,1]), np.max(visual_data_test[:,1])
xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))

# approximate Voronoi tesselation on resolution x resolution grid using 1-NN
background_model = KNeighborsClassifier(n_neighbors=1).fit(visual_data_test, torch.max(pred.data, 1).indices.cpu().numpy())
voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])
voronoiBackground = voronoiBackground.reshape((resolution, resolution))

#plot
plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.contour(xx, yy, voronoiBackground)

'''#if SAVE:
    #plt.savefig(DIR_NAME+"synthetic_dbtest0.pdf")'''

warmup_data = torch.concat([warmup_data,gradual_data[:100]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[:100]])

num_epochs = 101
core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)

core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

warmup_data = torch.concat([warmup_data,gradual_data[100:200]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[100:200]])

num_epochs = 101
core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)

core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

warmup_data = torch.concat([warmup_data,gradual_data[200:300]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[200:300]])
num_epochs = 101
core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)

core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

warmup_data = torch.concat([warmup_data,gradual_data[300:350]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[300:350]])
num_epochs = 101
core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)

core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

warmup_data = torch.concat([warmup_data,gradual_data[350:400]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[350:400]])
num_epochs = 101

core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)
core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

warmup_data = torch.concat([warmup_data,gradual_data[400:425]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[400:425]])
num_epochs = 101

core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)
core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

warmup_data = torch.concat([warmup_data,gradual_data[425:450]])
warmup_labels = torch.concat([warmup_labels,gradual_labels[425:450]])
num_epochs = 101

core1, spu1, loss_list1 = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)
core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

plt.rcParams.update({'figure.figsize': (8,4.5)})
plt.plot(spu.cpu(),linewidth=4,color="tab:blue",label='spurious')
plt.plot(core.cpu(),linewidth=4,color="tab:orange",label='core')
plt.plot(loss_list.cpu(),linewidth=3,color="tab:gray",linestyle='--',label='noise')
plt.ylim(0,2.4)
plt.xlabel('Number of iterations (t)', fontsize=25)
#plt.savefig(DIR_NAME+"warmup2.pdf")

pred = CNN(test_data)

resolution = 100 # 100x100 background pixels
X2d_xmin, X2d_xmax = np.min(visual_data_test[:,0]), np.max(visual_data_test[:,0])
X2d_ymin, X2d_ymax = np.min(visual_data_test[:,1]), np.max(visual_data_test[:,1])
xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))

# approximate Voronoi tesselation on resolution x resolution grid using 1-NN
background_model = KNeighborsClassifier(n_neighbors=1).fit(visual_data_test, torch.max(pred.data, 1).indices.cpu().numpy())
voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])
voronoiBackground = voronoiBackground.reshape((resolution, resolution))

#plot
plt.rcParams.update({'figure.figsize': (8,8)})
plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.contour(xx, yy, voronoiBackground)

#if SAVE:
    #plt.savefig(DIR_NAME+"synthetic_dbtest1.pdf")

test(CNN, criterion, test_data, test_labels)

large_test_data = torch.concat([test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==0)]],test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==1)]]])
large_test_label = torch.concat([test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==0)]],test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==1)]]])

test(CNN, criterion, large_test_data, large_test_label)

"""#Warmup + All"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import math
import numpy as np
import matplotlib.pyplot as plt
from math import log, e
import torch.optim as optim
import pickle
import sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

torch.manual_seed(22)
np.random.seed(22)

DIR_NAME = 'warmup+all/'
SAVE=True
GROUP_SIZE=0.98
NOISE=1/8

class ConvNet(nn.Module):
    def __init__(self, input_dim, out_channel, patch_num, small=True):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(1, out_channel*2, int(input_dim/patch_num), int(input_dim/patch_num))
        if small:
            self.conv1.weight = torch.nn.Parameter(self.conv1.weight*0.1)
            self.conv1.bias = torch.nn.Parameter(self.conv1.bias*0.1)
        self.out_channel = out_channel

    def forward(self, x):
        x = self.conv1(x)
        x = x**3
        x = torch.sum(x,2)
        output = torch.stack([torch.sum(x[:,:self.out_channel],1), torch.sum(x[:,self.out_channel:],1)]).transpose(1,0)
        return output

def train(model, criterion, data, labels, optimizers, epochs=100, plot=False, verbose=True):
    min_loss = 100
    core, spu, loss_list = [], [], []
    core_inner, spu_inner = test_inner(model)
    core.append(core_inner)
    spu.append(spu_inner)

    for epoch in range(epochs):

        for optimizer in optimizers:
            optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss_list.append(loss.detach())
        loss.backward()
        core_inner, spu_inner = test_inner(model)
        core.append(core_inner)
        spu.append(spu_inner)

        for optimizer in optimizers:
            optimizer.step()

        if epoch%100 == 0:
            if verbose:
                print('Epoch %d --- loss: %.3f' % (epoch + 1, loss.item()))

    core, spu, loss_list = torch.stack(core), torch.stack(spu), torch.stack(loss_list)
    print('Finished Training')
    return core, spu, loss_list


def test(model, criterion, data, labels):
    correct = 0
    with torch.no_grad():
        outputs = model(data)
        predicted = torch.max(outputs.data, 1).indices
        correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the %d test images: %.4f %%' % (data.shape[0],
        100 * correct / data.shape[0]))

    return 100 * correct / data.shape[0]


def test_inner(model):
    with torch.no_grad():
        core_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                      vc.float().unsqueeze(1).cuda())))
        spurious_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                          vs.float().unsqueeze(1).cuda())))

    return core_inner, spurious_inner

DATA_NUM = 10000
PATCH_NUM = 3
PATCH_LEN = 50

def div_norm(x):
    x /= np.linalg.norm(x)
    return x

vc = np.random.randn(PATCH_LEN)
vc /= np.linalg.norm(vc)

vs = np.random.randn(PATCH_LEN)
vs /= np.linalg.norm(vs)
vs -= vs.dot(vc) * vc
vs /= np.linalg.norm(vs)

vc, vs = torch.tensor(vc), torch.tensor(vs)

data = []
data_visual = []
labels = []
spurious_labels = []

xi_large, xi_small = torch.zeros(PATCH_LEN), torch.zeros(PATCH_LEN)

for i in range(DATA_NUM*2):
    y = np.random.choice([-1,1], 1)[0]
    alpha = np.random.uniform(0,1)

    # Feature noise patch
    a = (alpha<GROUP_SIZE)*y - (alpha>=GROUP_SIZE)*y
    # Noise patch
    xi = torch.tensor(np.random.normal(0, NOISE, size=(PATCH_LEN)))

    if a==y:
        xi_large+=xi
    else:
        xi_small+=xi

    core = vc*y/5
    spurious = vs*a
    x = torch.stack([core, spurious, xi])

    x_visual = torch.stack([core*1.2, spurious, xi])

    x = x.flatten()
    x_visual = x_visual.flatten()

    data.append(x)
    data_visual.append(x_visual)
    labels.append(y)
    spurious_labels.append(a)

data = torch.stack(data)
data_visual = torch.stack(data_visual)
print(data.shape)

labels = torch.tensor(labels)
spurious_labels = torch.tensor(spurious_labels)
labels[labels==-1] = 0
spurious_labels[spurious_labels==-1]=0
print(labels.shape)

training_data = data[:DATA_NUM,:].unsqueeze(1).float().cuda()
test_data = data[DATA_NUM:,:].unsqueeze(1).float().cuda()

print(training_data.shape, test_data.shape)

training_labels = labels[:DATA_NUM].cuda()
test_labels = labels[DATA_NUM:].cuda()
spurious_training_labels = spurious_labels[:DATA_NUM].cuda()
spurious_test_labels = spurious_labels[DATA_NUM:].cuda()
print(training_labels.shape, test_labels.shape)

training_labels = training_labels.long()
test_labels = test_labels.long()
spurious_training_labels = spurious_training_labels.long()
spurious_test_labels = spurious_test_labels.long()

pca = TSNE(n_components=2)

data_visual= pca.fit_transform(data_visual.float().cpu())
data_visual = data_visual/np.max(np.abs(data_visual))

visual_data_train = data_visual[:DATA_NUM,:]
visual_data_test = data_visual[DATA_NUM:,:]

for i in range(DATA_NUM):
    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([training_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    training_data[i,0,:] = stack_data

    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([test_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    test_data[i,0,:] = stack_data

small_test_data = torch.concat([test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])
small_test_label = torch.concat([test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])

smallest_group_size = min(len(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]]),len(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]]))

warmup_data = []
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][:smallest_group_size])
warmup_data = torch.concat(warmup_data)

warmup_data.shape

warmup_labels = []
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][:smallest_group_size])
warmup_labels = torch.concat(warmup_labels)

warmup_labels.shape

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda()
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)

num_epochs = 801
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.03, momentum=0.9)

core, spu, loss_list = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

pred = CNN(test_data)

resolution = 100 # 100x100 background pixels
X2d_xmin, X2d_xmax = np.min(visual_data_test[:,0]), np.max(visual_data_test[:,0])
X2d_ymin, X2d_ymax = np.min(visual_data_test[:,1]), np.max(visual_data_test[:,1])
xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))

# approximate Voronoi tesselation on resolution x resolution grid using 1-NN
background_model = KNeighborsClassifier(n_neighbors=1).fit(visual_data_test, torch.max(pred.data, 1).indices.cpu().numpy())
voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])
voronoiBackground = voronoiBackground.reshape((resolution, resolution))

#plot
plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.contour(xx, yy, voronoiBackground)

#if SAVE:
#    plt.savefig(DIR_NAME+"synthetic_dbtest0.pdf")

num_epochs = 701
plt.rcParams.update({'figure.figsize': (8,4.5)})

core1, spu1, loss_list1 = train(CNN, criterion, training_data, training_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

plt.plot(spu.cpu(),linewidth=4,color="tab:blue",label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_s\rangle$')
plt.plot(core.cpu(),linewidth=4,color="tab:orange",label=r'$\max_{j\in[J]}\langle\mathbf{w}_j^{(t)},\mathbf{v}_c\rangle$')
plt.plot(loss_list.cpu(),linewidth=4,color="tab:gray",linestyle='--',label='training loss')
plt.ylim(0,2.4)
plt.legend(prop={'size': 20},loc='center left')

if SAVE:
    plt.savefig(DIR_NAME+"warmup2.pdf")

pred = CNN(test_data)

resolution = 100 # 100x100 background pixels
X2d_xmin, X2d_xmax = np.min(visual_data_test[:,0]), np.max(visual_data_test[:,0])
X2d_ymin, X2d_ymax = np.min(visual_data_test[:,1]), np.max(visual_data_test[:,1])
xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))

# approximate Voronoi tesselation on resolution x resolution grid using 1-NN
background_model = KNeighborsClassifier(n_neighbors=1).fit(visual_data_test, torch.max(pred.data, 1).indices.cpu().numpy())
voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])
voronoiBackground = voronoiBackground.reshape((resolution, resolution))

#plot
plt.rcParams.update({'figure.figsize': (8,8)})
plt.scatter(visual_data_test[test_labels.cpu()==0][:,0],visual_data_test[test_labels.cpu()==0][:,1], s=20, marker='o',color='#d7191c')
plt.scatter(visual_data_test[test_labels.cpu()==1][:,0],visual_data_test[test_labels.cpu()==1][:,1], s=20, marker='o',color='#2b83ba')
plt.contour(xx, yy, voronoiBackground)

if SAVE:
    plt.savefig(DIR_NAME+"synthetic_dbtest1.pdf")

test(CNN, criterion, test_data, test_labels)

test(CNN, criterion, test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]], test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]])

test(CNN, criterion, test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]], test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]])

test(CNN, criterion, small_test_data, small_test_label)

"""#Warmup+All (no momentum after warmup)"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import math
import numpy as np
import matplotlib.pyplot as plt
from math import log, e
import torch.optim as optim
import pickle
import sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
plt.rcParams.update({'font.size': 20, 'figure.figsize': (8,4.5), 'font.family':'Arial', 'axes.axisbelow':True})

torch.manual_seed(22)
np.random.seed(22)

GROUP_SIZE=0.98
NOISE=1/8

class ConvNet(nn.Module):
    def __init__(self, input_dim, out_channel, patch_num, small=True):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(1, out_channel*2, int(input_dim/patch_num), int(input_dim/patch_num))
        if small:
            self.conv1.weight = torch.nn.Parameter(self.conv1.weight*0.1)
            self.conv1.bias = torch.nn.Parameter(self.conv1.bias*0.1)
        self.out_channel = out_channel

    def forward(self, x):
        x = self.conv1(x)
        x = x**3
        x = torch.sum(x,2)
        output = torch.stack([torch.sum(x[:,:self.out_channel],1), torch.sum(x[:,self.out_channel:],1)]).transpose(1,0)
        return output

def train(model, criterion, data, labels, optimizers, epochs=100, plot=False, verbose=True):
    min_loss = 100
    core, spu, loss_list = [], [], []
    core_inner, spu_inner = test_inner(model)
    core.append(core_inner)
    spu.append(spu_inner)

    for epoch in range(epochs):

        for optimizer in optimizers:
            optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss_list.append(loss.detach())
        loss.backward()
        core_inner, spu_inner = test_inner(model)
        core.append(core_inner)
        spu.append(spu_inner)

        for optimizer in optimizers:
            optimizer.step()

        if epoch%100 == 0:
            if verbose:
                print('Epoch %d --- loss: %.3f' % (epoch + 1, loss.item()))

    core, spu, loss_list = torch.stack(core), torch.stack(spu), torch.stack(loss_list)
    print('Finished Training')
    return core, spu, loss_list


def test(model, criterion, data, labels):
    correct = 0

    with torch.no_grad():
        outputs = model(data) # ,_
        predicted = torch.max(outputs.data, 1).indices
        correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the %d test images: %.4f %%' % (data.shape[0],
        100 * correct / data.shape[0]))

    return 100 * correct / data.shape[0]


def test_inner(model):
    with torch.no_grad():
        core_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                      vc.float().unsqueeze(1).cuda())))
        spurious_inner = torch.max(torch.abs(torch.matmul(model.conv1.weight.squeeze(1),
                                                          vs.float().unsqueeze(1).cuda())))

    return core_inner, spurious_inner

DATA_NUM = 10000
PATCH_NUM = 3
PATCH_LEN = 50

def div_norm(x):
    x /= np.linalg.norm(x)
    return x

vc = np.random.randn(PATCH_LEN)
vc /= np.linalg.norm(vc)

vs = np.random.randn(PATCH_LEN)
vs /= np.linalg.norm(vs)
vs -= vs.dot(vc) * vc
vs /= np.linalg.norm(vs)

vc, vs = torch.tensor(vc), torch.tensor(vs)

data = []
data_visual = []
labels = []
spurious_labels = []

xi_large, xi_small = torch.zeros(PATCH_LEN), torch.zeros(PATCH_LEN)

for i in range(DATA_NUM*2):
    y = np.random.choice([-1,1], 1)[0]
    alpha = np.random.uniform(0,1)

    # Feature noise patch
    a = (alpha<GROUP_SIZE)*y - (alpha>=GROUP_SIZE)*y
    # Noise patch
    xi = torch.tensor(np.random.normal(0, NOISE, size=(PATCH_LEN)))

    if a==y:
        xi_large+=xi
    else:
        xi_small+=xi

    core = vc*y/5
    spurious = vs*a
    x = torch.stack([core, spurious, xi])

    x_visual = torch.stack([core*1.2, spurious, xi])

    x = x.flatten()
    x_visual = x_visual.flatten()

    data.append(x)
    data_visual.append(x_visual)
    labels.append(y)
    spurious_labels.append(a)

data = torch.stack(data)
data_visual = torch.stack(data_visual)
print(data.shape)

labels = torch.tensor(labels)
spurious_labels = torch.tensor(spurious_labels)
labels[labels==-1] = 0
spurious_labels[spurious_labels==-1]=0
print(labels.shape)

training_data = data[:DATA_NUM,:].unsqueeze(1).float().cuda()
test_data = data[DATA_NUM:,:].unsqueeze(1).float().cuda()

print(training_data.shape, test_data.shape)

training_labels = labels[:DATA_NUM].cuda()
test_labels = labels[DATA_NUM:].cuda()
spurious_training_labels = spurious_labels[:DATA_NUM].cuda()
spurious_test_labels = spurious_labels[DATA_NUM:].cuda()
print(training_labels.shape, test_labels.shape)

training_labels = training_labels.long()
test_labels = test_labels.long()
spurious_training_labels = spurious_training_labels.long()
spurious_test_labels = spurious_test_labels.long()

for i in range(DATA_NUM):
    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([training_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    training_data[i,0,:] = stack_data

    idx = torch.randperm(PATCH_NUM)
    stack_data = torch.stack([test_data[i,0,:][j*PATCH_LEN:(j+1)*PATCH_LEN] for j in range(PATCH_NUM)])
    stack_data = stack_data[idx].flatten()
    test_data[i,0,:] = stack_data

small_test_data = torch.concat([test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])
small_test_label = torch.concat([test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]],test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]]])

smallest_group_size = min(len(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]]),len(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]]))

warmup_data = []
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]][:smallest_group_size])
warmup_data.append(training_data[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][:smallest_group_size])
warmup_data = torch.concat(warmup_data)

warmup_data.shape

warmup_labels = []
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==0)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==0)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==0,spurious_training_labels==1)]][:smallest_group_size])
warmup_labels.append(training_labels[[x and y for x,y in zip(training_labels==1,spurious_training_labels==1)]][:smallest_group_size])
warmup_labels = torch.concat(warmup_labels)

warmup_labels.shape

CNN = ConvNet(PATCH_LEN*PATCH_NUM, 20, PATCH_NUM).cuda() #, small=False
pred = CNN(test_data)
criterion = torch.nn.CrossEntropyLoss()
test(CNN, criterion, test_data, test_labels)

num_epochs = 801

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(CNN.parameters(), lr=0.03, momentum=0.9)

core, spu, loss_list = train(CNN, criterion, warmup_data, warmup_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

optimizer = torch.optim.SGD(CNN.parameters(), lr=0.03)

num_epochs = 701
plt.rcParams.update({'figure.figsize': (8,4.5)})

core1, spu1, loss_list1 = train(CNN, criterion, training_data, training_labels, [optimizer], num_epochs, plot=False)
test(CNN, criterion, test_data, test_labels)
test(CNN, criterion, small_test_data, small_test_label)

core = torch.concat([core,core1])
spu = torch.concat([spu,spu1])
loss_list = torch.concat([loss_list,loss_list1])

test(CNN, criterion, test_data, test_labels)

test(CNN, criterion, test_data[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]], test_labels[[x and y for x,y in zip(test_labels==1,spurious_test_labels==0)]])

test(CNN, criterion, test_data[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]], test_labels[[x and y for x,y in zip(test_labels==0,spurious_test_labels==1)]])

test(CNN, criterion, small_test_data, small_test_label)